{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing dependencies\n",
    "import wikipedia\n",
    "import pandas as pd\n",
    "from random import sample \n",
    "import time\n",
    "from math import log10,sqrt\n",
    "\n",
    "list_searches_nba = ['Los Angeles Lakers', 'Golden State Warriors', 'Toronto Raptors',\n",
    "                'Milwaukee Bucks', 'LA Clippers', 'Boston Celtics', 'Denver Nuggets',\n",
    "                'Utah Jazz', 'Miami Heat', 'Houston Rockets', 'Dallas Mavericks',\n",
    "                    'Atlanta Hawks', 'Brooklyn Nets', 'Charlotte Hornets', 'Chicago Bulls',\n",
    "                    'Cleveland Cavaliers', 'Detroit Pistons', 'Indiana Pacers', 'Memphis Grizzlies',\n",
    "                    'Minnesota Timberwolves', 'New Orleans Pelicans', 'New York Knicks', 'Oklahoma City Thunder']\n",
    "list_searches_cl = ['Manchester City', 'Paris Saint Germain', 'Real Madrid', 'FC Barcelona',\n",
    "                    'Liverpool', 'Juventus', 'Valancia', 'Atalanta Bergamo',\n",
    "                    'FC Bayern Munchen', 'Atl√©tico Madrid', 'SSC Napoli', 'Borussia Dortmund',\n",
    "                    'Olympique Lyon', 'Tottenham Hotspur', 'RasenBallsport Leipzig', 'Ajax Amsterdam',\n",
    "                    'Dinamo Zagreb', 'APOEL Nicosia', 'Dynamo kiev', 'Bayer 04 Leverkusen', 'Internazionale',\n",
    "                    'FC Red Bull Salzburg', 'Galatasaray', 'Olympiacos', 'Hoffenhheim', 'Schalke 04',\n",
    "                    'Feyenoord', 'PSV']\n",
    "\n",
    "# Function for extracting the wikipedia pages of teams\n",
    "def import_wikipedia(list_searches):\n",
    "    data_searches = []\n",
    "    for i in list_searches:\n",
    "        p = wikipedia.page(i)\n",
    "        data_searches.append(p.content)\n",
    "    return data_searches\n",
    "data_nba = import_wikipedia(list_searches_nba)\n",
    "data_cl = import_wikipedia(list_searches_cl)\n",
    "datasets = data_nba + data_cl\n",
    "\n",
    "lst_search = list_searches_nba + list_searches_cl\n",
    "stopwords= ['a','able','about','across','after','all','almost','also','am','among','an','and','any','are','as','at','be','because','been','but','by',\n",
    "            'can','cannot','could','dear','did','do','does','either','else','ever','every','for','from','get','got','had','has','have','he','her','hers',\n",
    "            'him','his','how','however','i','if','in','into','is','it','its','just','least','let','like','likely','may','me','might','most','must','my',\n",
    "            'neither','no','nor','not','of','off','often','on','only','or','other','our','own','rather','said','say','says','she','should','since','so',\n",
    "            'some','than','that','the','their','them','then','there','these','they','this','tis','to','too','twas','us','wants','was','we','were','what',\n",
    "            'when','where','which','while','who','whom','why','will','with','would','yet','you','your']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper1(data, lst_search):\n",
    "    final_output = []\n",
    "    for i in range(len(data)):\n",
    "        output_not_sorted = []\n",
    "        filename = lst_search[i]\n",
    "        # remove whitespace\n",
    "        line = data[i].strip()\n",
    "        # split into words\n",
    "        words = line.split()\n",
    "        for word in words:\n",
    "            word=word.lower()\n",
    "            if word not in stopwords:\n",
    "                z=word+' * '+filename\n",
    "                output = '%s \\t %s' % (z, 1)\n",
    "                output_not_sorted.append(output)\n",
    "        sorted_output = sorted(output_not_sorted)\n",
    "        final_output.append(sorted_output)\n",
    "\n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reducer1(map_result):\n",
    "    current_word = None\n",
    "    current_count = 0\n",
    "    word = None\n",
    "    final_output = []\n",
    "\n",
    "    for j in map_result:\n",
    "        for line in j:\n",
    "            line = line.strip()\n",
    "            word, count = line.split('\\t', 1)\n",
    "\n",
    "            try:\n",
    "                count = int(count)\n",
    "            except ValueError:\n",
    "                continue\n",
    "            \n",
    "            # The if-switch works because the output is mapped by key (here: word),\n",
    "            # this is how it also works in Hadoop\n",
    "            if current_word == word:\n",
    "                current_count += count\n",
    "            else:\n",
    "                if current_word:\n",
    "                    output = '%s \\t %s' % (current_word, current_count)\n",
    "                    final_output.append(output)\n",
    "                current_count = count\n",
    "                current_word = word\n",
    "\n",
    "    # Outputting the last word if needed\n",
    "    if current_word == word:\n",
    "        output = '%s \\t %s' % (current_word, current_count)\n",
    "        final_output.append(output)\n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.08 s, sys: 21.9 ms, total: 1.1 s\n",
      "Wall time: 1.11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "map_result1 = mapper1(datasets, lst_search)\n",
    "reduce_result1 = reducer1(map_result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper2(data_reducer):\n",
    "    final_output = []\n",
    "    for line in data_reducer:\n",
    "        # remove leading and trailing whitespace\n",
    "        line = line.strip()\n",
    "        # split the line into words\n",
    "        wordfilename,count=line.split('\\t',1)\n",
    "        wordfilename = wordfilename.strip()\n",
    "        try:\n",
    "            word,filename=wordfilename.split('*')\n",
    "        except:\n",
    "            pass\n",
    "        z=word+' '+count;\n",
    "        output = '%s \\t %s' % (filename, z)\n",
    "        final_output.append(output)\n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reducer2(map_result):\n",
    "    current_word = None\n",
    "    prev_filename = None\n",
    "    current_count = 0\n",
    "    word = None\n",
    "    N=0\n",
    "    df={}\n",
    "    l1=[]\n",
    "    final_output = []\n",
    "    \n",
    "    for line in map_result:\n",
    "        line = line.strip()\n",
    "        l1.append(line)\n",
    "        filename,wordcount = line.split('\\t', 1)\n",
    "        wordcount = wordcount.strip()\n",
    "        word,count = wordcount.split(' ', 1)\n",
    "        count=int(count)\n",
    "        if prev_filename == filename:\n",
    "            N+=count\n",
    "        else:\n",
    "            if prev_filename != None:\n",
    "                df[prev_filename]=N\n",
    "            N=0\n",
    "            prev_filename = filename\n",
    "    df[prev_filename]=N\n",
    "\n",
    "    for h in l1:\n",
    "        filename,wordcount = h.split('\\t', 1)\n",
    "        wordcount = wordcount.strip()\n",
    "        word,count = wordcount.split(' ', 1) \n",
    "        for k in df:\n",
    "            if filename == k:\n",
    "                wf=word+' '+filename\n",
    "                nN=count+' '+str(df[k])\n",
    "                output = '%s \\t %s' % (wf,nN)\n",
    "                final_output.append(output)\n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 751 ms, sys: 23.1 ms, total: 774 ms\n",
      "Wall time: 781 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "map_result2 = mapper2(reduce_result1)\n",
    "reduce_result2 = reducer2(map_result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper3(reduce_result):\n",
    "    output_list = []\n",
    "    for line in reduce_result:\n",
    "        line = line.strip()\n",
    "        wf,nN=line.split('\\t',1)\n",
    "        w,f=wf.split(' ',1)\n",
    "        z=f+' * '+nN+' '+str(1)\n",
    "        output = '%s \\t %s' % (w,z)\n",
    "        output_list.append(output)\n",
    "    final_output = sorted(output_list)\n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reducer3(map_result):\n",
    "    prev_word = None\n",
    "    count = 1 \n",
    "    word = None\n",
    "    final_output = []\n",
    "    for line in map_result:\n",
    "        line = line.strip()\n",
    "        w,z= line.split('\\t', 1)\n",
    "        w = w.strip()\n",
    "        z = z.strip()\n",
    "        f,nNc = z.split('*',1)\n",
    "        f = f.strip()\n",
    "        nNc = nNc.strip()\n",
    "        n,Nc=nNc.split(' ',1)\n",
    "        N,c=Nc.split(' ',1)\n",
    "        if prev_word == w:\n",
    "            count += int(c)\n",
    "        else:\n",
    "            if prev_word != None:\n",
    "                output=w+' '+f+' \\t '+n+' '+N+' '+str(count)\n",
    "                final_output.append(output)\n",
    "            count=1\n",
    "            prev_word = w\n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 216 ms, sys: 7.33 ms, total: 223 ms\n",
      "Wall time: 224 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "map_result3 = mapper3(reduce_result2)\n",
    "# reduce_result3 = reducer3(map_result3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 249 ms, sys: 5.73 ms, total: 255 ms\n",
      "Wall time: 254 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "reduce_result3 = reducer3(map_result3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper4(reduce_result):\n",
    "    D=51.0\n",
    "    final_output = []\n",
    "    # input comes from STDIN (standard input)\n",
    "    for line in reduce_result:\n",
    "        # remove leading and trailing whitespace\n",
    "        line = line.strip()\n",
    "        # split the line into words\n",
    "        wf,nNm=line.split('\\t',1)\n",
    "        wf = wf.strip()\n",
    "        nNm = nNm.strip()\n",
    "        n,N,m=nNm.split(' ',2)\n",
    "        n=float(n)\n",
    "        N=float(N)\n",
    "        m=float(m)\n",
    "        tfidf= (n/N)*log10(D/m)\n",
    "        output = '%s \\t %s' % (wf,tfidf)\n",
    "        final_output.append(output)\n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 134 ms, sys: 5.46 ms, total: 140 ms\n",
      "Wall time: 139 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "final = mapper4(reduce_result3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_computation_times(data_nba, data_cl):\n",
    "    running_times = []\n",
    "    for i in range(5, 51, 5):\n",
    "        new_data = [sample(data_nba,i), sample(data_cl,i)]\n",
    "        data1, data2 = compute_frequency_matrix(new_data)\n",
    "        start_time = time.time()\n",
    "        igm1, igm2 = compute_igm(data1, data2)\n",
    "        running_times.append(time.time() - start_time)\n",
    "    return running_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "running_times = []\n",
    "for i in range(5, 51, 5):\n",
    "    new_data = sample(datasets,i)\n",
    "    lst_searches = sample(lst_search, i)\n",
    "    start_time = time.time()\n",
    "    map_result1 = mapper1(new_data, lst_searches)\n",
    "    reduce_result1 = reducer1(map_result1)\n",
    "    map_result2 = mapper2(reduce_result1)\n",
    "    reduce_result2 = reducer2(map_result2)\n",
    "    map_result3 = mapper3(reduce_result2)\n",
    "    reduce_result3 = reducer3(map_result3)\n",
    "    final = mapper4(reduce_result3)\n",
    "    running_times.append(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.24024295806884766,\n",
       " 0.48619604110717773,\n",
       " 0.6151602268218994,\n",
       " 0.8494110107421875,\n",
       " 1.1304759979248047,\n",
       " 1.2444140911102295,\n",
       " 1.5537216663360596,\n",
       " 1.8164231777191162,\n",
       " 2.1589066982269287,\n",
       " 2.3509411811828613]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "running_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output = []\n",
    "final_tfidf = []\n",
    "D=51.0\n",
    "# input comes from STDIN (standard input)\n",
    "for line in reduce_result3:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    # split the line into words\n",
    "    wf,nNm=line.split('\\t',1)\n",
    "    wf = wf.strip()\n",
    "    w,f = wf.split(' ',1)\n",
    "    nNm = nNm.strip()\n",
    "    n,N,m=nNm.split(' ',2)\n",
    "    n=float(n)\n",
    "    N=float(N)\n",
    "    m=float(m)\n",
    "    tfidf= (n/N)*log10(D/m)\n",
    "    output = w\n",
    "    final_output.append(output)\n",
    "    final_tfidf.append(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16676</th>\n",
       "      <td>hawks</td>\n",
       "      <td>0.056722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22034</th>\n",
       "      <td>napoli</td>\n",
       "      <td>0.049946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25048</th>\n",
       "      <td>psp</td>\n",
       "      <td>0.046178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13160</th>\n",
       "      <td>dynamo</td>\n",
       "      <td>0.043161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15529</th>\n",
       "      <td>galatasaray</td>\n",
       "      <td>0.042655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7033</th>\n",
       "      <td>apoel</td>\n",
       "      <td>0.040384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27694</th>\n",
       "      <td>serie</td>\n",
       "      <td>0.036917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22902</th>\n",
       "      <td>olympiacos</td>\n",
       "      <td>0.032282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31743</th>\n",
       "      <td>valancia</td>\n",
       "      <td>0.031538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1602</th>\n",
       "      <td>(b.</td>\n",
       "      <td>0.027541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28224</th>\n",
       "      <td>sinsheim</td>\n",
       "      <td>0.027541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30002</th>\n",
       "      <td>team</td>\n",
       "      <td>0.023597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19252</th>\n",
       "      <td>lahore</td>\n",
       "      <td>0.022527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27079</th>\n",
       "      <td>salzburg</td>\n",
       "      <td>0.022158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14784</th>\n",
       "      <td>first</td>\n",
       "      <td>0.021328</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              word     tfidf\n",
       "16676        hawks  0.056722\n",
       "22034       napoli  0.049946\n",
       "25048          psp  0.046178\n",
       "13160       dynamo  0.043161\n",
       "15529  galatasaray  0.042655\n",
       "7033         apoel  0.040384\n",
       "27694        serie  0.036917\n",
       "22902   olympiacos  0.032282\n",
       "31743     valancia  0.031538\n",
       "1602           (b.  0.027541\n",
       "28224     sinsheim  0.027541\n",
       "30002         team  0.023597\n",
       "19252       lahore  0.022527\n",
       "27079     salzburg  0.022158\n",
       "14784        first  0.021328"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tuples = list(zip(final_output,final_tfidf))\n",
    "df1 = pd.DataFrame(data_tuples, columns=['word','tfidf'])\n",
    "df1.sort_values('tfidf', ascending=False).head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local TF-IDF\n",
    "[2.8284547328948975,\n",
    " 11.59837794303894,\n",
    " 12.069886922836304,\n",
    " 17.457234859466553,\n",
    " 28.922400951385498,\n",
    " 35.147900342941284,\n",
    " 44.212185859680176,\n",
    " 50.5285918712616,\n",
    " 57.81522607803345,\n",
    " 64.94160890579224]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel TF-IDF\n",
    "[2.1748712062835693,\n",
    " 3.716153621673584,\n",
    " 9.97753381729126,\n",
    " 14.972901582717896,\n",
    " 14.40234088897705,\n",
    " 24.855359315872192,\n",
    " 28.49886393547058,\n",
    " 30.41658616065979,\n",
    " 39.89217281341553,\n",
    " 45.6888325214386]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [5, 10, 15, 20, 25, 30, 35, 40, 45, 50]\n",
    "# kijken nog naar hoeveel bytes het is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99358"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(map_result3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34123"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reduce_result3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old Reducer\n",
    "# Gaat nog wat mis bij eerste prev_word = None en op het einde duurt die lang\n",
    "\n",
    "def reducer3(map_result):\n",
    "    prev_word = None\n",
    "    count = 1 \n",
    "    word = None\n",
    "    df={}\n",
    "    l1=[]\n",
    "    final_output = []\n",
    "    # input comes from STDIN\n",
    "    for line in map_result:\n",
    "        line = line.strip()\n",
    "        w,z= line.split('\\t', 1)\n",
    "        w = w.strip()\n",
    "        z = z.strip()\n",
    "        f,nNc = z.split('*',1)\n",
    "        f = f.strip()\n",
    "        nNc = nNc.strip()\n",
    "        n,Nc=nNc.split(' ',1)\n",
    "        N,c=Nc.split(' ',1)\n",
    "        if prev_word == w:\n",
    "            # count = count+int(c)\n",
    "            count += int(c)\n",
    "        else:\n",
    "            if prev_word != None:\n",
    "                q=n+' '+N+' '+str(count)\n",
    "                df[prev_word]=q\n",
    "                j=prev_word+' '+f\n",
    "                l1.append(j)\n",
    "            count=1\n",
    "            prev_word = w\n",
    "\n",
    "\n",
    "#     q=n+' '+N+' '+str(count)\n",
    "#     df[prev_word]=q\n",
    "#     j=prev_word+' '+f\n",
    "#     l1.append(j)\n",
    "\n",
    "    for h in l1:\n",
    "        w,f=h.split(' ',1)\n",
    "        w = w.strip()\n",
    "        f = f.strip()\n",
    "        for d in df:\n",
    "            if w == d:\n",
    "                output = '%s \\t %s' % (h,df[d])\n",
    "                final_output.append(output)\n",
    "    return final_output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
