{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing dependencies\n",
    "import wikipedia\n",
    "import pandas as pd\n",
    "from random import sample \n",
    "import time\n",
    "from math import log10,sqrt\n",
    "\n",
    "list_searches_nba = ['Los Angeles Lakers', 'Golden State Warriors', 'Toronto Raptors',\n",
    "                'Milwaukee Bucks', 'LA Clippers', 'Boston Celtics', 'Denver Nuggets',\n",
    "                'Utah Jazz', 'Miami Heat', 'Houston Rockets', 'Dallas Mavericks',\n",
    "                    'Atlanta Hawks', 'Brooklyn Nets', 'Charlotte Hornets', 'Chicago Bulls',\n",
    "                    'Cleveland Cavaliers', 'Detroit Pistons', 'Indiana Pacers', 'Memphis Grizzlies',\n",
    "                    'Minnesota Timberwolves', 'New Orleans Pelicans', 'New York Knicks', 'Oklahoma City Thunder']\n",
    "list_searches_cl = ['Manchester City', 'Paris Saint Germain', 'Real Madrid', 'FC Barcelona',\n",
    "                    'Liverpool', 'Juventus', 'Valancia', 'Atalanta Bergamo',\n",
    "                    'FC Bayern Munchen', 'Atl√©tico Madrid', 'SSC Napoli', 'Borussia Dortmund',\n",
    "                    'Olympique Lyon', 'Tottenham Hotspur', 'RasenBallsport Leipzig', 'Ajax Amsterdam',\n",
    "                    'Dinamo Zagreb', 'APOEL Nicosia', 'Dynamo kiev', 'Bayer 04 Leverkusen', 'Internazionale',\n",
    "                    'FC Red Bull Salzburg', 'Galatasaray', 'Olympiacos', 'Hoffenhheim', 'Schalke 04',\n",
    "                    'Feyenoord', 'PSV']\n",
    "\n",
    "# Function for extracting the wikipedia pages of teams\n",
    "def import_wikipedia(list_searches):\n",
    "    data_searches = []\n",
    "    for i in list_searches:\n",
    "        p = wikipedia.page(i)\n",
    "        data_searches.append(p.content)\n",
    "    return data_searches\n",
    "data_nba = import_wikipedia(list_searches_nba)\n",
    "data_cl = import_wikipedia(list_searches_cl)\n",
    "datasets = data_nba + data_cl\n",
    "\n",
    "lst_search = list_searches_nba + list_searches_cl\n",
    "stopwords= ['a','able','about','across','after','all','almost','also','am','among','an','and','any','are','as','at','be','because','been','but','by',\n",
    "            'can','cannot','could','dear','did','do','does','either','else','ever','every','for','from','get','got','had','has','have','he','her','hers',\n",
    "            'him','his','how','however','i','if','in','into','is','it','its','just','least','let','like','likely','may','me','might','most','must','my',\n",
    "            'neither','no','nor','not','of','off','often','on','only','or','other','our','own','rather','said','say','says','she','should','since','so',\n",
    "            'some','than','that','the','their','them','then','there','these','they','this','tis','to','too','twas','us','wants','was','we','were','what',\n",
    "            'when','where','which','while','who','whom','why','will','with','would','yet','you','your']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper1(data, lst_search):\n",
    "    final_output = []\n",
    "    for i in range(len(data)):\n",
    "        output_not_sorted = []\n",
    "        filename = lst_search[i]\n",
    "        # remove whitespace\n",
    "        line = data[i].strip()\n",
    "        # split into words\n",
    "        words = line.split()\n",
    "        for word in words:\n",
    "            word=word.lower()\n",
    "            if word not in stopwords:\n",
    "                z=word+' * '+filename\n",
    "                output = '%s \\t %s' % (z, 1)\n",
    "                output_not_sorted.append(output)\n",
    "        sorted_output = sorted(output_not_sorted)\n",
    "        final_output.append(sorted_output)\n",
    "\n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the term occurences n\n",
    "def reducer1(map_result):\n",
    "    current_word = None\n",
    "    current_count = 0\n",
    "    word = None\n",
    "    final_output = []\n",
    "\n",
    "    for j in map_result:\n",
    "        for line in j:\n",
    "            line = line.strip()\n",
    "            word, count = line.split('\\t', 1)\n",
    "\n",
    "            try:\n",
    "                count = int(count)\n",
    "            except ValueError:\n",
    "                continue\n",
    "            \n",
    "            # The if-switch works because the output is mapped by key (here: word),\n",
    "            # this is how it also works in Hadoop\n",
    "            if current_word == word:\n",
    "                current_count += count\n",
    "            else:\n",
    "                if current_word:\n",
    "                    output = '%s \\t %s' % (current_word, current_count)\n",
    "                    final_output.append(output)\n",
    "                current_count = count\n",
    "                current_word = word\n",
    "\n",
    "    # Outputting the last word if needed\n",
    "    if current_word == word:\n",
    "        output = '%s \\t %s' % (current_word, current_count)\n",
    "        final_output.append(output)\n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "map_result1 = mapper1(datasets, lst_search)\n",
    "reduce_result1 = reducer1(map_result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper2(data_reducer):\n",
    "    final_output = []\n",
    "    for line in data_reducer:\n",
    "        # remove whitespace\n",
    "        line = line.strip()\n",
    "        # splitting into words\n",
    "        wordfilename,count=line.split('\\t',1)\n",
    "        wordfilename = wordfilename.strip()\n",
    "        try:\n",
    "            word,filename=wordfilename.split('*')\n",
    "        except:\n",
    "            pass\n",
    "        z=word+' '+count;\n",
    "        output = '%s \\t %s' % (filename, z)\n",
    "        final_output.append(output)\n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the document lengths\n",
    "def reducer2(map_result):\n",
    "    current_word = None\n",
    "    prev_filename = None\n",
    "    current_count = 0\n",
    "    word = None\n",
    "    N=0\n",
    "    df={}\n",
    "    l1=[]\n",
    "    final_output = []\n",
    "    \n",
    "    for line in map_result:\n",
    "        line = line.strip()\n",
    "        l1.append(line)\n",
    "        filename,wordcount = line.split('\\t', 1)\n",
    "        wordcount = wordcount.strip()\n",
    "        word,count = wordcount.split(' ', 1)\n",
    "        count=int(count)\n",
    "        if prev_filename == filename:\n",
    "            N+=count\n",
    "        else:\n",
    "            if prev_filename != None:\n",
    "                df[prev_filename]=N\n",
    "            N=0\n",
    "            prev_filename = filename\n",
    "    df[prev_filename]=N\n",
    "\n",
    "    for h in l1:\n",
    "        filename,wordcount = h.split('\\t', 1)\n",
    "        wordcount = wordcount.strip()\n",
    "        word,count = wordcount.split(' ', 1) \n",
    "        for k in df:\n",
    "            if filename == k:\n",
    "                wf=word+' '+filename\n",
    "                nN=count+' '+str(df[k])\n",
    "                output = '%s \\t %s' % (wf,nN)\n",
    "                final_output.append(output)\n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "map_result2 = mapper2(reduce_result1)\n",
    "reduce_result2 = reducer2(map_result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting the words alphabetically\n",
    "def mapper3(reduce_result):\n",
    "    output_list = []\n",
    "    for line in reduce_result:\n",
    "        line = line.strip()\n",
    "        wf,nN=line.split('\\t',1)\n",
    "        w,f=wf.split(' ',1)\n",
    "        z=f+' * '+nN+' '+str(1)\n",
    "        output = '%s \\t %s' % (w,z)\n",
    "        output_list.append(output)\n",
    "    final_output = sorted(output_list)\n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the document-frequency of the terms\n",
    "def reducer3(map_result):\n",
    "    prev_word = None\n",
    "    count = 1 \n",
    "    word = None\n",
    "    final_output = []\n",
    "    for line in map_result:\n",
    "        line = line.strip()\n",
    "        w,z= line.split('\\t', 1)\n",
    "        w = w.strip()\n",
    "        z = z.strip()\n",
    "        f,nNc = z.split('*',1)\n",
    "        f = f.strip()\n",
    "        nNc = nNc.strip()\n",
    "        n,Nc=nNc.split(' ',1)\n",
    "        N,c=Nc.split(' ',1)\n",
    "        if prev_word == w:\n",
    "            count += int(c)\n",
    "        else:\n",
    "            if prev_word != None:\n",
    "                output=w+' '+f+' \\t '+n+' '+N+' '+str(count)\n",
    "                final_output.append(output)\n",
    "            count=1\n",
    "            prev_word = w\n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "map_result3 = mapper3(reduce_result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "reduce_result3 = reducer3(map_result3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the TF-IDF weights\n",
    "def mapper4(reduce_result):\n",
    "    D=51.0\n",
    "    final_output = []\n",
    "    for line in reduce_result:\n",
    "        # removing white space\n",
    "        line = line.strip()\n",
    "        # splitting the words\n",
    "        wf,nNm=line.split('\\t',1)\n",
    "        wf = wf.strip()\n",
    "        nNm = nNm.strip()\n",
    "        n,N,m=nNm.split(' ',2)\n",
    "        # transform to numeric for computation\n",
    "        n=float(n)\n",
    "        N=float(N)\n",
    "        m=float(m)\n",
    "        tfidf= (n/N)*log10(D/m)\n",
    "        output = '%s \\t %s' % (wf,tfidf)\n",
    "        final_output.append(output)\n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "final = mapper4(reduce_result3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_computation_times(data_nba, data_cl):\n",
    "    running_times = []\n",
    "    for i in range(5, 51, 5):\n",
    "        new_data = [sample(data_nba,i), sample(data_cl,i)]\n",
    "        data1, data2 = compute_frequency_matrix(new_data)\n",
    "        start_time = time.time()\n",
    "        igm1, igm2 = compute_igm(data1, data2)\n",
    "        running_times.append(time.time() - start_time)\n",
    "    return running_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "running_times = []\n",
    "for i in range(5, 51, 5):\n",
    "    new_data = sample(datasets,i)\n",
    "    lst_searches = sample(lst_search, i)\n",
    "    start_time = time.time()\n",
    "    map_result1 = mapper1(new_data, lst_searches)\n",
    "    reduce_result1 = reducer1(map_result1)\n",
    "    map_result2 = mapper2(reduce_result1)\n",
    "    reduce_result2 = reducer2(map_result2)\n",
    "    map_result3 = mapper3(reduce_result2)\n",
    "    reduce_result3 = reducer3(map_result3)\n",
    "    final = mapper4(reduce_result3)\n",
    "    running_times.append(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output = []\n",
    "final_tfidf = []\n",
    "D=51.0\n",
    "# input comes from STDIN (standard input)\n",
    "for line in reduce_result3:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    # split the line into words\n",
    "    wf,nNm=line.split('\\t',1)\n",
    "    wf = wf.strip()\n",
    "    w,f = wf.split(' ',1)\n",
    "    nNm = nNm.strip()\n",
    "    n,N,m=nNm.split(' ',2)\n",
    "    n=float(n)\n",
    "    N=float(N)\n",
    "    m=float(m)\n",
    "    tfidf= (n/N)*log10(D/m)\n",
    "    output = w\n",
    "    final_output.append(output)\n",
    "    final_tfidf.append(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tuples = list(zip(final_output,final_tfidf))\n",
    "df1 = pd.DataFrame(data_tuples, columns=['word','tfidf'])\n",
    "df1.sort_values('tfidf', ascending=False).head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For plotting Figures\n",
    "# Local vs Local\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "docs = [0.4, 0.8, 1.2, 1.6, 2.0, 2.4, 2.8, 3.2, 3.6, 4.0] # beginnen bij 0-\n",
    "local_tfidf = [0.2649269104003906,\n",
    " 0.3808310031890869,\n",
    " 0.6321542263031006,\n",
    " 0.887855052947998,\n",
    " 1.214566946029663,\n",
    " 1.4615440368652344,\n",
    " 1.629607915878296,\n",
    " 1.8721039295196533,\n",
    " 1.9866352081298828,\n",
    " 2.4448893070220947]\n",
    "local_tfigm = [0.37888002395629883,\n",
    " 0.8000800609588623,\n",
    " 0.8628139495849609,\n",
    " 1.406703233718872,\n",
    " 1.6783900260925293,\n",
    " 2.091007947921753,\n",
    " 2.395679235458374,\n",
    " 2.5690276622772217,\n",
    " 3.1481950283050537,\n",
    " 3.3638057708740234]\n",
    "\n",
    "plt.plot(docs, local_tfidf, 'o-')\n",
    "plt.plot(docs, local_tfigm, 'o-')\n",
    "# plt.plot(x, 2 * x)\n",
    "# plt.plot(x, 3 * x)\n",
    "# plt.plot(x, 4 * x)\n",
    "\n",
    "plt.title('Local TFIDF vs Local TFIGM', fontsize=18)\n",
    "plt.legend(['y = TFIDF', 'y = TFIGM'], loc='upper left')\n",
    "plt.xlabel('Size in Megabytes', fontsize=18)\n",
    "plt.ylabel('Time (in seconds)', fontsize=16)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel vs Parallel\n",
    "docs = [0.4, 0.8, 1.2, 1.6, 2.0, 2.4, 2.8, 3.2, 3.6, 4.0] # beginnen bij 0-\n",
    "parallel_tfidf = [0.22279024124145508,\n",
    " 0.41489458084106445,\n",
    " 0.6862201690673828,\n",
    " 0.7408955097198486,\n",
    " 1.0512993335723877,\n",
    " 1.1799609661102295,\n",
    " 1.5287902355194092,\n",
    " 1.686872959136963,\n",
    " 1.8508782386779785,\n",
    " 2.123838186264038]\n",
    "parallel_tfigm = [0.4660379886627197,\n",
    " 0.8262436389923096,\n",
    " 0.8983533382415771,\n",
    " 1.3413560390472412,\n",
    " 1.4024183750152588,\n",
    " 1.7298922538757324,\n",
    " 2.121650457382202,\n",
    " 2.3955447673797607,\n",
    " 2.6349055767059326,\n",
    " 3.0996642112731934]\n",
    "\n",
    "plt.plot(docs, parallel_tfidf, 'o-')\n",
    "plt.plot(docs, parallel_tfigm, 'o-')\n",
    "# plt.plot(x, 2 * x)\n",
    "# plt.plot(x, 3 * x)\n",
    "# plt.plot(x, 4 * x)\n",
    "plt.yticks([0.5, 1, 1.5, 2, 2.5, 3, 3.5])\n",
    "\n",
    "plt.title('Parallel TFIDF vs Parallel TFIGM', fontsize=18)\n",
    "plt.legend(['y = TFIDF', 'y = TFIGM'], loc='upper left')\n",
    "plt.xlabel('Size in Megabytes', fontsize=18)\n",
    "plt.ylabel('Time (in seconds)', fontsize=16)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local vs Parallel IGM\n",
    "docs = [0.4, 0.8, 1.2, 1.6, 2.0, 2.4, 2.8, 3.2, 3.6, 4.0] # beginnen bij 0-\n",
    "local_tfigm = [0.37888002395629883,\n",
    " 0.8000800609588623,\n",
    " 0.8628139495849609,\n",
    " 1.406703233718872,\n",
    " 1.6783900260925293,\n",
    " 2.091007947921753,\n",
    " 2.395679235458374,\n",
    " 2.5690276622772217,\n",
    " 3.1481950283050537,\n",
    " 3.3638057708740234]\n",
    "parallel_tfigm = [0.4660379886627197,\n",
    " 0.8262436389923096,\n",
    " 0.8983533382415771,\n",
    " 1.3413560390472412,\n",
    " 1.4024183750152588,\n",
    " 1.7298922538757324,\n",
    " 2.121650457382202,\n",
    " 2.3955447673797607,\n",
    " 2.6349055767059326,\n",
    " 3.0996642112731934]\n",
    "\n",
    "plt.plot(docs, local_tfigm, 'o-')\n",
    "plt.plot(docs, parallel_tfigm, 'o-')\n",
    "# plt.plot(x, 2 * x)\n",
    "# plt.plot(x, 3 * x)\n",
    "# plt.plot(x, 4 * x)\n",
    "\n",
    "plt.title('Local TFIGM vs Parallel TFIGM', fontsize=18)\n",
    "plt.legend(['y = Local', 'y = Parallel'], loc='upper left')\n",
    "plt.xlabel('Size in Megabytes', fontsize=18)\n",
    "plt.ylabel('Time (in seconds)', fontsize=16)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
