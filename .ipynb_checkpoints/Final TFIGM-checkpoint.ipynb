{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import wikipedia\n",
    "import pandas as pd\n",
    "from random import sample \n",
    "import time\n",
    "from math import log10,sqrt\n",
    "\n",
    "# Searched for random nba and 'big' football teams\n",
    "list_searches_nba = ['Los Angeles Lakers', 'Golden State Warriors', 'Toronto Raptors',\n",
    "                'Milwaukee Bucks', 'LA Clippers', 'Boston Celtics', 'Denver Nuggets',\n",
    "                'Utah Jazz', 'Miami Heat', 'Houston Rockets', 'Dallas Mavericks',\n",
    "                    'Atlanta Hawks', 'Brooklyn Nets', 'Charlotte Hornets', 'Chicago Bulls',\n",
    "                    'Cleveland Cavaliers', 'Detroit Pistons', 'Indiana Pacers', 'Memphis Grizzlies',\n",
    "                    'Minnesota Timberwolves', 'New Orleans Pelicans', 'New York Knicks', 'Oklahoma City Thunder',\n",
    "                    'San Antonio Spurs', 'Orlando Magic']\n",
    "list_searches_cl = ['Manchester City', 'Paris Saint Germain', 'Real Madrid', 'FC Barcelona',\n",
    "                    'Liverpool', 'Juventus', 'Valancia', 'Atalanta Bergamo',\n",
    "                    'FC Bayern Munchen', 'Atl√©tico Madrid', 'SSC Napoli', 'Borussia Dortmund',\n",
    "                    'Olympique Lyon', 'Tottenham Hotspur', 'RasenBallsport Leipzig', 'Ajax Amsterdam',\n",
    "                    'Dinamo Zagreb', 'APOEL Nicosia', 'Dynamo kiev', 'Bayer 04 Leverkusen', 'Internazionale',\n",
    "                    'FC Red Bull Salzburg', 'Galatasaray', 'Olympiacos', 'Hoffenhheim', 'Schalke 04']\n",
    "\n",
    "# Function for importaning the pages\n",
    "def import_wikipedia(list_searches):\n",
    "    data_searches, count = [], 0\n",
    "    for search in list_searches:\n",
    "        try:\n",
    "            p = wikipedia.page(search)\n",
    "            data_searches.append(p.content)\n",
    "        except:\n",
    "            count += 1\n",
    "    return data_searches, count\n",
    "\n",
    "data_nba, inv_nba = import_wikipedia(list_searches_nba)\n",
    "data_cl, inv_cl = import_wikipedia(list_searches_cl)\n",
    "\n",
    "# List of stopwords for removing stopwords\n",
    "stopwords= ['a','able','about','across','after','all','almost','also','am','among','an','and','any','are','as','at','be','because','been','but','by',\n",
    "            'can','cannot','could','dear','did','do','does','either','else','ever','every','for','from','get','got','had','has','have','he','her','hers',\n",
    "            'him','his','how','however','i','if','in','into','is','it','its','just','least','let','like','likely','may','me','might','most','must','my',\n",
    "            'neither','no','nor','not','of','off','often','on','only','or','other','our','own','rather','said','say','says','she','should','since','so',\n",
    "            'some','than','that','the','their','them','then','there','these','they','this','tis','to','too','twas','us','wants','was','we','were','what',\n",
    "            'when','where','which','while','who','whom','why','will','with','would','yet','you','your']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for determining the classes based on filenames\n",
    "class1 = list_searches_nba\n",
    "class2 = list_searches_cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first reducer, counts the words, term n (global)\n",
    "def reducer1(map_result):\n",
    "    current_word = None\n",
    "    current_count = 0\n",
    "    word = None\n",
    "    final_output = []\n",
    "\n",
    "    for j in map_result:\n",
    "        for line in j:\n",
    "            line = line.strip()\n",
    "            # because the list is sorted alphabetically, this works (this is also identical behavior in MapReduce for Hadoop)\n",
    "            word, count = line.split('\\t', 1)\n",
    "            try:\n",
    "                count = int(count)\n",
    "            except ValueError:\n",
    "                continue\n",
    "                \n",
    "            if current_word == word:\n",
    "                current_count += count\n",
    "            else:\n",
    "                if current_word:\n",
    "                    output = '%s \\t %s' % (current_word, current_count)\n",
    "                    final_output.append(output)\n",
    "                current_count = count\n",
    "                current_word = word\n",
    "    # Appending the word counts\n",
    "    if current_word == word:\n",
    "        output = '%s \\t %s' % (current_word, current_count)\n",
    "        final_output.append(output)\n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# map_result1 = mapper1(datasets, lst_search)\n",
    "# reduce_result1 = reducer1(map_result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second mapper arranges the documents for computhing N (total number of words in document)\n",
    "def mapper2(data_reducer):\n",
    "    final_output = []\n",
    "    for line in data_reducer:\n",
    "        # remove leading and trailing whitespace\n",
    "        line = line.strip()\n",
    "        # split the line into words\n",
    "        wordfilename,count=line.split('\\t',1)\n",
    "        wordfilename = wordfilename.strip()\n",
    "        try:\n",
    "            class_word,filename=wordfilename.split('*')\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            clss,word=class_word.split('|')\n",
    "        except:\n",
    "            pass\n",
    "        word = word.strip()\n",
    "        z=word+' '+count+' | '+clss;\n",
    "        output = '%s \\t %s' % (filename, z)\n",
    "        final_output.append(output)\n",
    "\n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computes N (total words in a document) by summing the n's per document\n",
    "def reducer2(map_result):\n",
    "    current_word = None\n",
    "    prev_filename = None\n",
    "    current_count = 0\n",
    "    word = None\n",
    "    N=0\n",
    "    df={}\n",
    "    l1=[]\n",
    "    final_output = []\n",
    "\n",
    "    for line in map_result:\n",
    "        line = line.strip()\n",
    "        l1.append(line)\n",
    "        filename,wordcount = line.split('\\t', 1)\n",
    "        wordcount_clss = wordcount.strip()\n",
    "        wordcount,clss = wordcount.split('|', 1)\n",
    "        word,count = wordcount.split()\n",
    "        word = word.strip()\n",
    "        clss = clss.strip()\n",
    "        clss=int(clss)\n",
    "        count = count.strip()\n",
    "        count=int(count)\n",
    "        if prev_filename == filename:\n",
    "            N+=count\n",
    "        else:\n",
    "            if prev_filename != None:\n",
    "                df[prev_filename]=N\n",
    "            N=0\n",
    "            prev_filename = filename\n",
    "    df[prev_filename]=N\n",
    "\n",
    "    for h in l1:\n",
    "        filename,wordcount = h.split('\\t', 1)\n",
    "        wordcount_clss = wordcount.strip()\n",
    "        wordcount,clss = wordcount.split('|', 1)\n",
    "        word,count = wordcount.split()\n",
    "        for k in df:\n",
    "            if filename == k:\n",
    "                wf=word+' '+filename\n",
    "                nN=count+' '+str(df[k])+'|'+clss\n",
    "                output = '%s \\t %s' % (wf,nN)\n",
    "                final_output.append(output)\n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# map_result2 = mapper2(reduce_result1)\n",
    "# reduce_result2 = reducer2(map_result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third mapper arranges words again, so class specific document frequencies can be computed\n",
    "def mapper3(reduce_result):\n",
    "    output_list = []\n",
    "    for line in reduce_result:\n",
    "        line = line.strip()\n",
    "        wf,nN_clss=line.split('\\t',1)\n",
    "        w,f=wf.split(' ',1)\n",
    "        nN_clss = nN_clss.strip()\n",
    "        nN, clss = nN_clss.split('|')\n",
    "        z=f+' * '+nN+' '+str(1)+' | '+clss\n",
    "        output = '%s \\t %s' % (w,z)\n",
    "        output_list.append(output)\n",
    "    final_output = sorted(output_list)\n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class-specific document frequency\n",
    "def reducer3(map_result):\n",
    "    lst1 = []\n",
    "    lst2 = []\n",
    "    final_output = []\n",
    "    prev_word = None\n",
    "    count = 1 \n",
    "    word = None\n",
    "    for line in map_result:\n",
    "        line = line.strip()\n",
    "        w,clss = line.split('|')\n",
    "        w=w.strip()\n",
    "        clss=clss.strip()\n",
    "        clss=int(clss)\n",
    "        if clss==1:\n",
    "            output = w+' | '+str(clss)\n",
    "            lst1.append(output)\n",
    "        else:\n",
    "            output = w+' | '+str(clss)\n",
    "            lst2.append(output)\n",
    "    lst_total = [lst1+lst2][0]\n",
    "\n",
    "    prev_word = None\n",
    "    count = 1 \n",
    "    word = None\n",
    "    final_output = []\n",
    "    for line in lst_total:\n",
    "        line = line.strip()\n",
    "        w,z= line.split('\\t', 1)\n",
    "        w = w.strip()\n",
    "        z = z.strip()\n",
    "        f,nNc_clss = z.split('*',1)\n",
    "        f = f.strip()\n",
    "        nNc_clss = nNc_clss.strip()\n",
    "        nNc,clss=nNc_clss.split('|',1)\n",
    "        nNc.strip()\n",
    "        clss.strip()\n",
    "        n,Nc=nNc.split(' ',1)\n",
    "        N,c=Nc.split(' ',1)\n",
    "        if prev_word == w:\n",
    "            count += int(c)\n",
    "        else:\n",
    "            if prev_word != None:\n",
    "                output=w+' '+f+' \\t '+n+' '+N+' '+str(count)+' | '+str(clss)\n",
    "                final_output.append(output)\n",
    "            count=1\n",
    "            prev_word = w\n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# map_result3 = mapper3(reduce_result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# reduce_result3 = reducer3(map_result3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting the words for adding the class-specific document frequencies together\n",
    "def mapper4(reduce_result):\n",
    "    counts_clss1 = []\n",
    "    wfs_clss1 = []\n",
    "    counts_clss2 = []\n",
    "    wfs_clss2 = []\n",
    "    final_output = []\n",
    "    for line in reduce_result:\n",
    "        line = line.strip()\n",
    "        wf_nNc, clss = line.split('|',1)\n",
    "        wf,nNc = wf_nNc.split('\\t',1)\n",
    "        nNc = nNc.strip()\n",
    "        n,N,fkr = nNc.split(' ')\n",
    "        clss = clss.strip()\n",
    "        clss = int(clss)\n",
    "        wf = wf.strip()\n",
    "        w,f = wf.split(' ',1)\n",
    "        output = w+ ' '+str(clss)+' \\t '+n+' '+N+' \\\\t ' +f+' \\\\\\t '+fkr\n",
    "        final_output.append(output)\n",
    "    final_output = sorted(final_output)\n",
    "    \n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map_result4 = mapper4(reduce_result3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the class-specific document frequencies together\n",
    "def reducer4(map_result):\n",
    "    final_output = []\n",
    "    final_output2 = []\n",
    "    prev_word = None\n",
    "    prev_fkr = '1'\n",
    "    for line in map_result:\n",
    "        line = line.strip()\n",
    "        w_class, nN_doc_fkr = line.split('\\t',1)\n",
    "        nN_doc, fkr = nN_doc_fkr.split('\\\\\\t',1)\n",
    "        fkr = fkr.strip()\n",
    "        w_class = w_class.strip()\n",
    "        w,clss = w_class.split(' ', 1)\n",
    "        w = w.strip()\n",
    "        if prev_word == w:\n",
    "            total = str(fkr)+' '+str(prev_fkr)\n",
    "        elif (w == prev_word and len(str(prev_fkr)) == 1):\n",
    "            total = str(fkr)+' '+str(prev_fkr)\n",
    "        else:\n",
    "            total = str(fkr)\n",
    "        prev_fkr = fkr\n",
    "        prev_word = w\n",
    "        output = w_class+' \\t '+nN_doc+' \\\\\\t '+str(total)\n",
    "        final_output.append(output)\n",
    "\n",
    "    prev_word = None\n",
    "    prev_fkr = '1'\n",
    "    for line in reversed(final_output):\n",
    "        line = line.strip()\n",
    "        w_class, nN_doc_fkr = line.split('\\t',1)\n",
    "        nN_doc, fkr = nN_doc_fkr.split('\\\\\\t',1)\n",
    "        fkr = fkr.strip()\n",
    "        w_class = w_class.strip()\n",
    "        w,clss = w_class.split(' ', 1)\n",
    "        w = w.strip()\n",
    "        if (prev_word == w and len(str(prev_fkr)) > 1):\n",
    "            total = str(prev_fkr)\n",
    "        else:\n",
    "            total = str(fkr)\n",
    "        prev_fkr = fkr\n",
    "        prev_word = w\n",
    "        output = w_class+' \\t '+nN_doc+' \\\\\\t '+str(total)\n",
    "        final_output2.append(output)\n",
    "    return final_output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce_result4 = reducer4(map_result4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the TF-IGM values per term\n",
    "def mapper5(data_reducer):\n",
    "    lmbd = 1.5\n",
    "    final_output = []\n",
    "    output_wclass = []\n",
    "    output_igm = []\n",
    "    for line in data_reducer:    \n",
    "        line = line.strip()\n",
    "        rest, fkrm = line.split('\\\\\\t',1)\n",
    "        fkrm = fkrm.strip()\n",
    "        w_clss_nN,doc =  rest.split('\\\\t',1)\n",
    "        w_clss_nN = w_clss_nN.strip()\n",
    "        w_class, nN = w_clss_nN.split('\\t',1)\n",
    "        nN = nN.strip()\n",
    "        n, N = nN.split(' ', 1)\n",
    "        n = int(n)\n",
    "        N = int(N)\n",
    "        fkrm = fkrm.strip()\n",
    "        if len(fkrm) > 2:\n",
    "            if \" \" in fkrm:\n",
    "                fkr1, fkr2 = fkrm.split(\" \", 1)\n",
    "                fkr1, fkr2 = int(fkr1), int(fkr2)\n",
    "                if N == 0:\n",
    "                    N = 1\n",
    "                igm = n/N * (1+lmbd*(max([fkr1, fkr2])/sum([fkr1+fkr2])))\n",
    "            else:\n",
    "                igm = 0\n",
    "        else:\n",
    "            if N == 0:\n",
    "                N = 1\n",
    "            igm = (n/N)*(1+lmbd*1)\n",
    "        output = w_class+ ' '+doc+' \\t '+str(igm)\n",
    "#         output_wclass.append(w_class)\n",
    "#         output_igm.append(igm)\n",
    "        final_output.append(output)\n",
    "        \n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_tfigm(data): #, searches):\n",
    "    map_result1 = mapper1(data) #, searches)\n",
    "    reduce_result1 = reducer1(map_result1)\n",
    "    map_result2 = mapper2(reduce_result1)\n",
    "    reduce_result2 = reducer2(map_result2)\n",
    "    map_result3 = mapper3(reduce_result2)\n",
    "    reduce_result3 = reducer3(map_result3)\n",
    "    map_result4 = mapper4(reduce_result3)\n",
    "    reduce_result4 = reducer4(map_result4)\n",
    "    final = mapper5(reduce_result4)\n",
    "    \n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = data_nba*100 + data_cl*100\n",
    "\n",
    "# Merging the lists\n",
    "lst_search = list_searches_nba + list_searches_cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from functools import partial\n",
    "from multiprocessing import Pool\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First mapper function, attaching class, removing stopwords, and sorting the words (alphabetically)\n",
    "def mapper1(data):\n",
    "    final_output = []\n",
    "    for i in range(len(data)):\n",
    "        output_not_sorted = []\n",
    "        filename = sample(lst_search, 1)[0]\n",
    "#         filename = lst_search[i]\n",
    "        if filename in class1:\n",
    "            given_class = 1\n",
    "        else:\n",
    "            given_class = 2\n",
    "#         given_class = sample([1, 2], 1)\n",
    "        \n",
    "        # Remove white space\n",
    "        line = data[i].strip()\n",
    "        # split into words\n",
    "        words = line.split()\n",
    "        for word in words:\n",
    "            word=word.lower()\n",
    "            if word not in stopwords:\n",
    "                # attaching the class\n",
    "                z=str(given_class)+' | '+word+' * '+filename\n",
    "                output = '%s \\t %s' % (z, 1)\n",
    "                output_not_sorted.append(output)\n",
    "        sorted_output = sorted(output_not_sorted)\n",
    "        final_output.append(sorted_output)\n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oke\n"
     ]
    }
   ],
   "source": [
    "print(\"oke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.329134225845337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-19:\n",
      "Process ForkPoolWorker-18:\n",
      "Process ForkPoolWorker-21:\n",
      "Process ForkPoolWorker-20:\n",
      "Process ForkPoolWorker-17:\n",
      "Process ForkPoolWorker-16:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"<ipython-input-37-ae1a8439d47c>\", line 7, in parallel_tfigm\n",
      "    reduce_result3 = reducer3(map_result3)\n",
      "  File \"<ipython-input-37-ae1a8439d47c>\", line 2, in parallel_tfigm\n",
      "    map_result1 = mapper1(data) #, searches)\n",
      "  File \"<ipython-input-37-ae1a8439d47c>\", line 7, in parallel_tfigm\n",
      "    reduce_result3 = reducer3(map_result3)\n",
      "  File \"<ipython-input-37-ae1a8439d47c>\", line 2, in parallel_tfigm\n",
      "    map_result1 = mapper1(data) #, searches)\n",
      "  File \"<ipython-input-37-ae1a8439d47c>\", line 2, in parallel_tfigm\n",
      "    map_result1 = mapper1(data) #, searches)\n",
      "  File \"<ipython-input-37-ae1a8439d47c>\", line 6, in parallel_tfigm\n",
      "    map_result3 = mapper3(reduce_result2)\n",
      "  File \"<ipython-input-29-1eab9db56cfa>\", line 12, in reducer3\n",
      "    w=w.strip()\n",
      "  File \"<ipython-input-28-a31d022074f1>\", line 6, in mapper3\n",
      "    wf,nN_clss=line.split('\\t',1)\n",
      "  File \"<ipython-input-40-7d47f8203fa6>\", line 6, in mapper1\n",
      "    filename = sample(lst_search, 1)[0]\n",
      "  File \"<ipython-input-40-7d47f8203fa6>\", line 20, in mapper1\n",
      "    if word not in stopwords:\n",
      "  File \"<ipython-input-29-1eab9db56cfa>\", line 12, in reducer3\n",
      "    w=w.strip()\n",
      "  File \"<ipython-input-40-7d47f8203fa6>\", line 6, in mapper1\n",
      "    filename = sample(lst_search, 1)[0]\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/anaconda3/lib/python3.7/random.py\", line 318, in sample\n",
      "    randbelow = self._randbelow\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/anaconda3/lib/python3.7/random.py\", line 338, in sample\n",
      "    while j in selected:\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-d6039edfcd26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#         func = partial(parallel_tfigm, new_data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparallel_tfigm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mrunning_times\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mthat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         '''\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstarmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    652\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 648\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with Pool(processes=6) as pool:\n",
    "    running_times = []\n",
    "    for i in range(100, 1100, 100):\n",
    "        new_data = datasets[:i]# sample(datasets,i)\n",
    "#         lst_searches = lst_search[:i] # sample(lst_search, i)\n",
    "        start_time = time.time()\n",
    "#         func = partial(parallel_tfigm, new_data)\n",
    "        pool.map(parallel_tfigm, new_data)\n",
    "        print(time.time() - start_time)\n",
    "        running_times.append(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "700"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local TF-IGM\n",
    "57.18224501609802\n",
    "112.47146010398865\n",
    "165.73978686332703\n",
    "221.77069091796875\n",
    "270.4036648273468\n",
    "320.8091850280762\n",
    "400.92387413978577\n",
    "451.65632700920105\n",
    "511.9483811855316\n",
    "555.0262262821198"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel TF-IGM\n",
    "10.912653923034668\n",
    "25.431442975997925\n",
    "36.806010246276855\n",
    "47.119266748428345\n",
    "56.70264911651611\n",
    "70.51972198486328\n",
    "81.978276014328\n",
    "91.14404082298279\n",
    "120.51627683639526\n",
    "121.98761200904846"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400.92387413978577\n",
      "451.65632700920105\n",
      "511.9483811855316\n",
      "555.0262262821198\n"
     ]
    }
   ],
   "source": [
    "with Pool(processes=1) as pool:\n",
    "    running_times = []\n",
    "    for i in range(700, 1100, 100):\n",
    "        new_data = datasets[:i]# sample(datasets,i)\n",
    "#         lst_searches = lst_search[:i] # sample(lst_search, i)\n",
    "        start_time = time.time()\n",
    "#         func = partial(parallel_tfigm, new_data)\n",
    "        pool.map(parallel_tfigm, new_data)\n",
    "        print(time.time() - start_time)\n",
    "        running_times.append(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.912653923034668\n",
      "25.431442975997925\n",
      "36.806010246276855\n",
      "47.119266748428345\n",
      "56.70264911651611\n",
      "70.51972198486328\n",
      "81.978276014328\n",
      "91.14404082298279\n",
      "120.51627683639526\n",
      "121.98761200904846\n"
     ]
    }
   ],
   "source": [
    "with Pool(processes=6) as pool:\n",
    "    running_times = []\n",
    "    for i in range(100, 1100, 100):\n",
    "        new_data = datasets[:i]# sample(datasets,i)\n",
    "#         lst_searches = lst_search[:i] # sample(lst_search, i)\n",
    "        start_time = time.time()\n",
    "#         func = partial(parallel_tfigm, new_data)\n",
    "        pool.map(parallel_tfigm, new_data)\n",
    "        print(time.time() - start_time)\n",
    "        running_times.append(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = datasets[:900]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "900"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 cores: 245.01554584503174\n",
    "# 3 cores: 169.88558411598206\n",
    "# 4 cores: 137.16499018669128\n",
    "# 5 cores: 123.00809288024902"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137.16499018669128\n",
      "123.00809288024902\n"
     ]
    }
   ],
   "source": [
    "for i in range(4, 6, 1):\n",
    "    with Pool(processes=i) as pool:\n",
    "        start_time = time.time()\n",
    "        result = pool.map(parallel_tfigm, new_data)\n",
    "        print(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138.08856916427612\n",
      "278.4285681247711\n",
      "416.02373123168945\n",
      "555.573157787323\n",
      "720.0035171508789\n",
      "870.6720798015594\n",
      "991.5431771278381\n",
      "1151.6833758354187\n",
      "1274.6585099697113\n",
      "1409.7844321727753\n"
     ]
    }
   ],
   "source": [
    "with Pool(processes=1) as pool:\n",
    "    running_times = []\n",
    "    for i in range(100, 1100, 100):\n",
    "        new_data = datasets[:i]# sample(datasets,i)\n",
    "#         lst_searches = lst_search[:i] # sample(lst_search, i)\n",
    "        start_time = time.time()\n",
    "#         func = partial(parallel_tfigm, new_data)\n",
    "        pool.map(parallel_tfigm, new_data)\n",
    "        print(time.time() - start_time)\n",
    "        running_times.append(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parallel (6 cores)\n",
    "30.135493993759155\n",
    "54.48365592956543\n",
    "80.22214913368225\n",
    "106.95011878013611\n",
    "136.95192098617554\n",
    "168.36995792388916\n",
    "195.76798677444458\n",
    "221.30671095848083\n",
    "253.11553406715393\n",
    "289.38061785697937"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local 1 core\n",
    "138.08856916427612\n",
    "278.4285681247711\n",
    "416.02373123168945\n",
    "555.573157787323\n",
    "720.0035171508789\n",
    "870.6720798015594\n",
    "991.5431771278381\n",
    "1151.6833758354187\n",
    "1274.6585099697113\n",
    "1409.7844321727753"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2.1351287364959717\n",
    "2.1325666904449463\n",
    "2.2169289588928223\n",
    "2.276876926422119\n",
    "2.427320957183838\n",
    "2.4833920001983643\n",
    "2.4856011867523193\n",
    "2.629715919494629"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1.1243817806243896\n",
    "1.223736047744751\n",
    "1.2502851486206055\n",
    "1.3273632526397705\n",
    "1.3809452056884766\n",
    "1.4344279766082764\n",
    "1.507964849472046\n",
    "1.619715690612793"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %%time\n",
    "# running_times = []\n",
    "# for i in range(5, 45, 5): # 51\n",
    "#     new_data = sample(datasets,i)\n",
    "#     lst_searches = sample(lst_search, i)\n",
    "#     start_time = time.time()\n",
    "#     map_result1 = mapper1(new_data, lst_searches)\n",
    "#     reduce_result1 = reducer1(map_result1)\n",
    "#     map_result2 = mapper2(reduce_result1)\n",
    "#     reduce_result2 = reducer2(map_result2)\n",
    "#     map_result3 = mapper3(reduce_result2)\n",
    "#     reduce_result3 = reducer3(map_result3)\n",
    "#     map_result4 = mapper4(reduce_result3)\n",
    "#     reduce_result4 = reducer4(map_result4)\n",
    "#     final = mapper5(reduce_result4)\n",
    "#     running_times.append(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.2533600330352783,\n",
       " 0.45272302627563477,\n",
       " 0.5005459785461426,\n",
       " 0.7690169811248779,\n",
       " 0.9512901306152344,\n",
       " 1.1218140125274658,\n",
       " 1.3915767669677734,\n",
       " 1.5869920253753662]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "running_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
