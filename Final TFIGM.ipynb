{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import wikipedia\n",
    "import pandas as pd\n",
    "from random import sample \n",
    "import time\n",
    "from math import log10,sqrt\n",
    "\n",
    "# Searched for random nba and 'big' football teams\n",
    "list_searches_nba = ['Los Angeles Lakers', 'Golden State Warriors', 'Toronto Raptors',\n",
    "                'Milwaukee Bucks', 'LA Clippers', 'Boston Celtics', 'Denver Nuggets',\n",
    "                'Utah Jazz', 'Miami Heat', 'Houston Rockets', 'Dallas Mavericks',\n",
    "                    'Atlanta Hawks', 'Brooklyn Nets', 'Charlotte Hornets', 'Chicago Bulls',\n",
    "                    'Cleveland Cavaliers', 'Detroit Pistons', 'Indiana Pacers', 'Memphis Grizzlies',\n",
    "                    'Minnesota Timberwolves', 'New Orleans Pelicans', 'New York Knicks', 'Oklahoma City Thunder',\n",
    "                    'San Antonio Spurs', 'Orlando Magic']\n",
    "list_searches_cl = ['Manchester City', 'Paris Saint Germain', 'Real Madrid', 'FC Barcelona',\n",
    "                    'Liverpool', 'Juventus', 'Valancia', 'Atalanta Bergamo',\n",
    "                    'FC Bayern Munchen', 'Atl√©tico Madrid', 'SSC Napoli', 'Borussia Dortmund',\n",
    "                    'Olympique Lyon', 'Tottenham Hotspur', 'RasenBallsport Leipzig', 'Ajax Amsterdam',\n",
    "                    'Dinamo Zagreb', 'APOEL Nicosia', 'Dynamo kiev', 'Bayer 04 Leverkusen', 'Internazionale',\n",
    "                    'FC Red Bull Salzburg', 'Galatasaray', 'Olympiacos', 'Hoffenhheim', 'Schalke 04']\n",
    "\n",
    "# Function for importaning the pages\n",
    "def import_wikipedia(list_searches):\n",
    "    data_searches = []\n",
    "    for i in list_searches:\n",
    "        p = wikipedia.page(i)\n",
    "        data_searches.append(p.content)\n",
    "    return data_searches\n",
    "data_nba = import_wikipedia(list_searches_nba)\n",
    "data_cl = import_wikipedia(list_searches_cl)\n",
    "datasets = data_nba + data_cl\n",
    "\n",
    "# Merging the lists\n",
    "lst_search = list_searches_nba + list_searches_cl\n",
    "\n",
    "# List of stopwords for removing stopwords\n",
    "stopwords= ['a','able','about','across','after','all','almost','also','am','among','an','and','any','are','as','at','be','because','been','but','by',\n",
    "            'can','cannot','could','dear','did','do','does','either','else','ever','every','for','from','get','got','had','has','have','he','her','hers',\n",
    "            'him','his','how','however','i','if','in','into','is','it','its','just','least','let','like','likely','may','me','might','most','must','my',\n",
    "            'neither','no','nor','not','of','off','often','on','only','or','other','our','own','rather','said','say','says','she','should','since','so',\n",
    "            'some','than','that','the','their','them','then','there','these','they','this','tis','to','too','twas','us','wants','was','we','were','what',\n",
    "            'when','where','which','while','who','whom','why','will','with','would','yet','you','your']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for determining the classes based on filenames\n",
    "class1 = list_searches_nba\n",
    "class2 = list_searches_cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First mapper function, attaching class, removing stopwords, and sorting the words (alphabetically)\n",
    "def mapper1(data, lst_search):\n",
    "    final_output = []\n",
    "    for i in range(len(data)):\n",
    "        output_not_sorted = []\n",
    "        filename = lst_search[i]\n",
    "        if filename in class1:\n",
    "            given_class = 1\n",
    "        else:\n",
    "            given_class = 2\n",
    "        # Remove white space\n",
    "        line = data[i].strip()\n",
    "        # split into words\n",
    "        words = line.split()\n",
    "        for word in words:\n",
    "            word=word.lower()\n",
    "            if word not in stopwords:\n",
    "                # attaching the class\n",
    "                z=str(given_class)+' | '+word+' * '+filename\n",
    "                output = '%s \\t %s' % (z, 1)\n",
    "                output_not_sorted.append(output)\n",
    "        sorted_output = sorted(output_not_sorted)\n",
    "        final_output.append(sorted_output)\n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first reducer, counts the words, term n (global)\n",
    "def reducer1(map_result):\n",
    "    current_word = None\n",
    "    current_count = 0\n",
    "    word = None\n",
    "    final_output = []\n",
    "\n",
    "    for j in map_result:\n",
    "        for line in j:\n",
    "            line = line.strip()\n",
    "            # because the list is sorted alphabetically, this works (this is also identical behavior in MapReduce for Hadoop)\n",
    "            word, count = line.split('\\t', 1)\n",
    "            try:\n",
    "                count = int(count)\n",
    "            except ValueError:\n",
    "                continue\n",
    "                \n",
    "            if current_word == word:\n",
    "                current_count += count\n",
    "            else:\n",
    "                if current_word:\n",
    "                    output = '%s \\t %s' % (current_word, current_count)\n",
    "                    final_output.append(output)\n",
    "                current_count = count\n",
    "                current_word = word\n",
    "    # Appending the word counts\n",
    "    if current_word == word:\n",
    "        output = '%s \\t %s' % (current_word, current_count)\n",
    "        final_output.append(output)\n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.17 s, sys: 21.8 ms, total: 1.19 s\n",
      "Wall time: 1.19 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "map_result1 = mapper1(datasets, lst_search)\n",
    "reduce_result1 = reducer1(map_result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second mapper arranges the documents for computhing N (total number of words in document)\n",
    "def mapper2(data_reducer):\n",
    "    final_output = []\n",
    "    for line in reduce_result1:\n",
    "        # remove leading and trailing whitespace\n",
    "        line = line.strip()\n",
    "        # split the line into words\n",
    "        wordfilename,count=line.split('\\t',1)\n",
    "        wordfilename = wordfilename.strip()\n",
    "        try:\n",
    "            class_word,filename=wordfilename.split('*')\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            clss,word=class_word.split('|')\n",
    "        except:\n",
    "            pass\n",
    "        word = word.strip()\n",
    "        z=word+' '+count+' | '+clss;\n",
    "        output = '%s \\t %s' % (filename, z)\n",
    "        final_output.append(output)\n",
    "\n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computes N (total words in a document) by summing the n's per document\n",
    "def reducer2(map_result):\n",
    "    current_word = None\n",
    "    prev_filename = None\n",
    "    current_count = 0\n",
    "    word = None\n",
    "    N=0\n",
    "    df={}\n",
    "    l1=[]\n",
    "    final_output = []\n",
    "\n",
    "    for line in map_result2:\n",
    "        line = line.strip()\n",
    "        l1.append(line)\n",
    "        filename,wordcount = line.split('\\t', 1)\n",
    "        wordcount_clss = wordcount.strip()\n",
    "        wordcount,clss = wordcount.split('|', 1)\n",
    "        word,count = wordcount.split()\n",
    "        word = word.strip()\n",
    "        clss = clss.strip()\n",
    "        clss=int(clss)\n",
    "        count = count.strip()\n",
    "        count=int(count)\n",
    "        if prev_filename == filename:\n",
    "            N+=count\n",
    "        else:\n",
    "            if prev_filename != None:\n",
    "                df[prev_filename]=N\n",
    "            N=0\n",
    "            prev_filename = filename\n",
    "    df[prev_filename]=N\n",
    "\n",
    "    for h in l1:\n",
    "        filename,wordcount = h.split('\\t', 1)\n",
    "        wordcount_clss = wordcount.strip()\n",
    "        wordcount,clss = wordcount.split('|', 1)\n",
    "        word,count = wordcount.split()\n",
    "        for k in df:\n",
    "            if filename == k:\n",
    "                wf=word+' '+filename\n",
    "                nN=count+' '+str(df[k])+'|'+clss\n",
    "                output = '%s \\t %s' % (wf,nN)\n",
    "                final_output.append(output)\n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 930 ms, sys: 25.4 ms, total: 955 ms\n",
      "Wall time: 957 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "map_result2 = mapper2(reduce_result1)\n",
    "reduce_result2 = reducer2(map_result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third mapper arranges words again, so class specific document frequencies can be computed\n",
    "def mapper3(reduce_result):\n",
    "    output_list = []\n",
    "    for line in reduce_result:\n",
    "        line = line.strip()\n",
    "        wf,nN_clss=line.split('\\t',1)\n",
    "        w,f=wf.split(' ',1)\n",
    "        nN_clss = nN_clss.strip()\n",
    "        nN, clss = nN_clss.split('|')\n",
    "        z=f+' * '+nN+' '+str(1)+' | '+clss\n",
    "        output = '%s \\t %s' % (w,z)\n",
    "        output_list.append(output)\n",
    "    final_output = sorted(output_list)\n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class-specific document frequency\n",
    "def reducer3(map_result):\n",
    "    lst1 = []\n",
    "    lst2 = []\n",
    "    final_output = []\n",
    "    prev_word = None\n",
    "    count = 1 \n",
    "    word = None\n",
    "    for line in map_result3:\n",
    "        line = line.strip()\n",
    "        w,clss = line.split('|')\n",
    "        w=w.strip()\n",
    "        clss=clss.strip()\n",
    "        clss=int(clss)\n",
    "        if clss==1:\n",
    "            output = w+' | '+str(clss)\n",
    "            lst1.append(output)\n",
    "        else:\n",
    "            output = w+' | '+str(clss)\n",
    "            lst2.append(output)\n",
    "    lst_total = [lst1+lst2][0]\n",
    "\n",
    "    prev_word = None\n",
    "    count = 1 \n",
    "    word = None\n",
    "    final_output = []\n",
    "    for line in lst_total:\n",
    "        line = line.strip()\n",
    "        w,z= line.split('\\t', 1)\n",
    "        w = w.strip()\n",
    "        z = z.strip()\n",
    "        f,nNc_clss = z.split('*',1)\n",
    "        f = f.strip()\n",
    "        nNc_clss = nNc_clss.strip()\n",
    "        nNc,clss=nNc_clss.split('|',1)\n",
    "        nNc.strip()\n",
    "        clss.strip()\n",
    "        n,Nc=nNc.split(' ',1)\n",
    "        N,c=Nc.split(' ',1)\n",
    "        if prev_word == w:\n",
    "            count += int(c)\n",
    "        else:\n",
    "            if prev_word != None:\n",
    "                output=w+' '+f+' \\t '+n+' '+N+' '+str(count)+' | '+str(clss)\n",
    "                final_output.append(output)\n",
    "            count=1\n",
    "            prev_word = w\n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 270 ms, sys: 8.61 ms, total: 279 ms\n",
      "Wall time: 279 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "map_result3 = mapper3(reduce_result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 512 ms, sys: 18.7 ms, total: 530 ms\n",
      "Wall time: 537 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "reduce_result3 = reducer3(map_result3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting the words for adding the class-specific document frequencies together\n",
    "def mapper4(reduce_result):\n",
    "    counts_clss1 = []\n",
    "    wfs_clss1 = []\n",
    "    counts_clss2 = []\n",
    "    wfs_clss2 = []\n",
    "    final_output = []\n",
    "    for line in reduce_result:\n",
    "        line = line.strip()\n",
    "        wf_nNc, clss = line.split('|',1)\n",
    "        wf,nNc = wf_nNc.split('\\t',1)\n",
    "        nNc = nNc.strip()\n",
    "        n,N,fkr = nNc.split(' ')\n",
    "        clss = clss.strip()\n",
    "        clss = int(clss)\n",
    "        wf = wf.strip()\n",
    "        w,f = wf.split(' ',1)\n",
    "        output = w+ ' '+str(clss)+' \\t '+n+' '+N+' \\\\t ' +f+' \\\\\\t '+fkr\n",
    "        final_output.append(output)\n",
    "    final_output = sorted(final_output)\n",
    "    \n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_result4 = mapper4(reduce_result3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the class-specific document frequencies together\n",
    "def reducer4(map_result):\n",
    "    final_output = []\n",
    "    final_output2 = []\n",
    "    prev_word = None\n",
    "    prev_fkr = '1'\n",
    "    for line in map_result:\n",
    "        line = line.strip()\n",
    "        w_class, nN_doc_fkr = line.split('\\t',1)\n",
    "        nN_doc, fkr = nN_doc_fkr.split('\\\\\\t',1)\n",
    "        fkr = fkr.strip()\n",
    "        w_class = w_class.strip()\n",
    "        w,clss = w_class.split(' ', 1)\n",
    "        w = w.strip()\n",
    "        if prev_word == w:\n",
    "            total = str(fkr)+' '+str(prev_fkr)\n",
    "        elif (w == prev_word and len(str(prev_fkr)) == 1):\n",
    "            total = str(fkr)+' '+str(prev_fkr)\n",
    "        else:\n",
    "            total = str(fkr)\n",
    "        prev_fkr = fkr\n",
    "        prev_word = w\n",
    "        output = w_class+' \\t '+nN_doc+' \\\\\\t '+str(total)\n",
    "        final_output.append(output)\n",
    "\n",
    "    prev_word = None\n",
    "    prev_fkr = '1'\n",
    "    for line in reversed(final_output):\n",
    "        line = line.strip()\n",
    "        w_class, nN_doc_fkr = line.split('\\t',1)\n",
    "        nN_doc, fkr = nN_doc_fkr.split('\\\\\\t',1)\n",
    "        fkr = fkr.strip()\n",
    "        w_class = w_class.strip()\n",
    "        w,clss = w_class.split(' ', 1)\n",
    "        w = w.strip()\n",
    "        if (prev_word == w and len(str(prev_fkr)) > 1):\n",
    "            total = str(prev_fkr)\n",
    "        else:\n",
    "            total = str(fkr)\n",
    "        prev_fkr = fkr\n",
    "        prev_word = w\n",
    "        output = w_class+' \\t '+nN_doc+' \\\\\\t '+str(total)\n",
    "        final_output2.append(output)\n",
    "    return final_output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_result4 = reducer4(map_result4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the TF-IGM values per term\n",
    "def mapper5(data_reducer):\n",
    "    final_output = []\n",
    "    output_wclass = []\n",
    "    output_igm = []\n",
    "    for line in data_reducer:    \n",
    "        line = line.strip()\n",
    "        rest, fkrm = line.split('\\\\\\t',1)\n",
    "        fkrm = fkrm.strip()\n",
    "        w_clss_nN,doc =  rest.split('\\\\t',1)\n",
    "        w_clss_nN = w_clss_nN.strip()\n",
    "        w_class, nN = w_clss_nN.split('\\t',1)\n",
    "        nN = nN.strip()\n",
    "        n, N = nN.split(' ', 1)\n",
    "        n = int(n)\n",
    "        N = int(N)\n",
    "        fkrm = fkrm.strip()\n",
    "        if len(fkrm) > 2:\n",
    "            fkr1, fkr2 = fkrm.split(' ', 1)\n",
    "            fkr1, fkr2 = int(fkr1), int(fkr2)\n",
    "            igm = n/N * (max([fkr1, fkr2])/sum([fkr1+fkr2]))\n",
    "        else:\n",
    "            igm = n/N\n",
    "        output = w_class+ ' '+doc+' \\t '+str(igm)\n",
    "#         output_wclass.append(w_class)\n",
    "#         output_igm.append(igm)\n",
    "        final_output.append(output)\n",
    "        \n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "running_times = []\n",
    "for i in range(5, 51, 5):\n",
    "    new_data = sample(datasets,i)\n",
    "    lst_searches = sample(lst_search, i)\n",
    "    start_time = time.time()\n",
    "    map_result1 = mapper1(new_data, lst_searches)\n",
    "    reduce_result1 = reducer1(map_result1)\n",
    "    map_result2 = mapper2(reduce_result1)\n",
    "    reduce_result2 = reducer2(map_result2)\n",
    "    map_result3 = mapper3(reduce_result2)\n",
    "    reduce_result3 = reducer3(map_result3)\n",
    "    map_result4 = mapper4(reduce_result3)\n",
    "    reduce_result4 = reducer4(map_result4)\n",
    "    final = mapper5(reduce_result4)\n",
    "    running_times.append(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.37888002395629883,\n",
       " 0.8000800609588623,\n",
       " 0.8628139495849609,\n",
       " 1.406703233718872,\n",
       " 1.6783900260925293,\n",
       " 2.091007947921753,\n",
       " 2.395679235458374,\n",
       " 2.5690276622772217,\n",
       " 3.1481950283050537,\n",
       " 3.3638057708740234]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "running_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipped = list(zip(output_wclass, output_igm))\n",
    "df = pd.DataFrame(zipped, columns=['word', 'igm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values('igm', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
